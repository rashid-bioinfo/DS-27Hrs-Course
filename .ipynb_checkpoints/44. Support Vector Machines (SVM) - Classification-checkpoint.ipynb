{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044c9da-fcd9-486d-ba92-4236e83aba0f",
   "metadata": {},
   "source": [
    "# 44. Support Vector Machines (SVM) - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303abff-fe88-43be-b4b2-4e7f999eda8c",
   "metadata": {},
   "source": [
    "-- SVM is one of the most popular **Supervised Learning** algorithm, which is used fro classification as well as Regression problems\n",
    "- With the help of SVM, you can handle both linear and non-linear data\n",
    "- Its working is similar to logistic regression algo\n",
    "- **Algo of SVM**:\n",
    "1) It finds two points (support vectors) in the data (shown in red color in below figure)\n",
    "2) It passes marginal plan/line (hyperplane) from these support vectors (dotted red lines)\n",
    "3) It measures the distance b/w these two lines\n",
    "4) Take avearge of the distance (divided by 2) \n",
    "5) This average is denoted by a separable line (Maximum margin) (solid red line)\n",
    "6) Prediction is done through this line and decided the new data would go to which category (Splitting of data)\n",
    "7) The distance (d) b/w 2 vectors should be maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50d850-04b4-411e-9f27-37396a2234d5",
   "metadata": {},
   "source": [
    "<img src=\"Images/svm.jpg\"  style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b193d-d797-478c-a1d6-67bccaef2dd4",
   "metadata": {},
   "source": [
    "- **Hard Margin:** The algorithm aims to find a hyperplane that perfectly separates the data into two classes w/o any misclassifications.\n",
    "- **Soft Margin:** The algorithm allows for some misclassifications to find a hyperplane that generalizes better to unseen data and is more robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0564f1-85a5-42bd-9368-c3e6bd8f5458",
   "metadata": {},
   "source": [
    "<img src=\"Images/svm-2.jpg\"  style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a43c44-9824-4a71-9f8a-e3c1c18b148d",
   "metadata": {},
   "source": [
    "<img src=\"Images/svm-3.jpg\"  style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1af43d-eb16-47f9-a1e3-9b3e16b654cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87bbb4d2-a2d5-4bda-b239-a158a3b238f7",
   "metadata": {},
   "source": [
    "**Types of SVM:** There are two different types of SVMs, each used for different things:\n",
    "\n",
    "1) **Simple SVM:** Typically used for linear regression and classifications problems.\n",
    "2) **Kernel SVM:** Has more flexibility for non-linear data b/c you can add more features to fit a hyperplane instead of a two-dimensional space.\n",
    "- Kernel SVM is used when our data is not linearly separable.\n",
    "-  It modifies our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8ffdd-06a2-45d9-9730-a1ea23356a14",
   "metadata": {},
   "source": [
    "**Kernel Functions:**\n",
    "- Kernel functions play a crucial role in transforming input into a higher-dimensional space.\n",
    "- The primary purpose of kernel functions is to allow SVMs to handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space where linear separation may be more feasible.\n",
    "- This transformation is done w/o explicitly calculating the coordinate points in that higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163dc4e-23ba-4b94-975f-33914586e1dd",
   "metadata": {},
   "source": [
    "### Kernel Functions in SVM\n",
    "\n",
    "1. **Linear Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = x_i^T x_j\n",
    "   $$\n",
    "\n",
    "2. **Polynomial Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = (\\gamma \\cdot x_i^T x_j + r)^d\n",
    "   $$\n",
    "\n",
    "3. **Gaussian Radial Basis Function (RBF) Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
    "   $$\n",
    "\n",
    "4. **Sigmoid Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = \\tanh(\\gamma \\cdot x_i^T x_j + r)\n",
    "   $$\n",
    "\n",
    "### Description of Symbols\n",
    "- \\( x_i, x_j \\): Input feature vectors.\n",
    "- \\( x_i^T x_j \\): Dot product between vectors \\(x_i\\) and \\(x_j\\).\n",
    "- \\( \\gamma \\): Scaling factor (often related to \\(\\sigma\\) in the Gaussian RBF kernel as \\(\\gamma = \\frac{1}{2\\sigma^2}\\)).\n",
    "- \\( r \\): Constant term (bias).\n",
    "- \\( d \\): Degree of the polynomial in the Polynomial Kernel.\n",
    "- \\( \\|x_i - x_j\\| \\): Euclidean distance between \\(x_i\\) and \\(x_j\\).\n",
    "- \\( \\exp \\): Exponential function.\n",
    "- \\( \\tanh \\): Hyperbolic tangent function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecad6a9-4d4f-4bb3-a82d-fbed4c5581f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
