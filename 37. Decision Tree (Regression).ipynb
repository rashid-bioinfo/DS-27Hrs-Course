{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a3633b-409a-4e9e-ba0d-bcf22c642fd3",
   "metadata": {},
   "source": [
    "# 37. Decision Tree (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1e015-4bcb-4a0f-bef2-ef81afe00fb2",
   "metadata": {},
   "source": [
    "**Decision Tree**\n",
    "- Decision Tree is a **Supervised Learning** technique that can be used for both classification and regression problems, but mostly it is preferred for solving **classification problems**\n",
    "- In order to build a tree, we can use the **CART algorithm**, which stands for Classification and Regression Tree algorithm\n",
    "- it splits your data (Binary splitting)\n",
    "- It works on non-linear splitting data\n",
    "- It works as a conditional statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0790a-17c2-4efd-b886-44571e567d19",
   "metadata": {},
   "source": [
    "**Important Terminology related to Decision Tree**\n",
    "\n",
    "- **Root Node:** It represents the entire population or sample and this further gets divided into two or more homogenous sets\n",
    "- **Splitting:** It is a process of dividing a node into two or more sub-nodes\n",
    "- **Decision Node:** When a sub-node splits into furhter sub-nodes\n",
    "- **Leaf/Terminal Node:** Nodes do not split further\n",
    "- **Pruning:** When we remove sub-nodes of a decision node, this is an opposite process of splitting. Some time tree bcome too big, so the chances of over-fitting. So to avoid over-ftting, we use pruning\n",
    "- **Branch/Sub-Tree:** A subsection of the entire tree\n",
    "- **Parent and child node:** A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c2179-c99b-48e5-b775-83255bcbfe7d",
   "metadata": {},
   "source": [
    "<img src=\"Images/decision-tree.jpg\"  style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d879ce1-7c2e-49ad-b0c2-98ca2fc31a88",
   "metadata": {},
   "source": [
    "In below example, we can split the tree from:\n",
    "1) company\n",
    "2) Job\n",
    "3) Degree\n",
    "- However, we will consider the factors (which are explained below) to decide from which node, we should start splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb8d07-e636-4663-95c7-6738b0d55604",
   "metadata": {},
   "source": [
    "<img src=\"Images/decision-tree-2.jpg\"  style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d89b4-d21e-43a3-8519-53bc2c0a310e",
   "metadata": {},
   "source": [
    "### Absolute Selection Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e6f5b-3a71-478d-bca2-bb373fb22460",
   "metadata": {},
   "source": [
    "This measurement, we can easily select the best attribute for the nodes of the tree. There are two popular techniques for ASM, which are:\n",
    "1) Information Gain\n",
    "2) Entropy / Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73627c1b-cbe2-4816-904b-60302f812eca",
   "metadata": {},
   "source": [
    "**Entropy**:\n",
    "Entropy is a metric to measure the impurity in a given attribute. it specifies randomness in data.\n",
    "$$\n",
    "\\text{Entropy(s) = - P(yes) log2 P(yes) - P(no) Log2 P(no)}\n",
    "$$\n",
    "Where:\n",
    "- S = Total number of samples\n",
    "- P(yes) = Probability of yes\n",
    "- P(no) = Probability of no\n",
    "\n",
    "**Information Gain**:\n",
    "Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute. It calculates how much information a feature provides us about a class.\n",
    "$$\n",
    "\\text{Information Gain = Entropy(S) - [(Weighted Avg) * Entropy(each feature)]}\n",
    "$$\n",
    "**Gini Index**:\n",
    "Gin index is a measure of impurity or purity used while creating a decision tree in the CART (Classification and Regression Tree) algorithm.\n",
    "An attribute with the low Gini index should be preferred as compared to the high Gini Index\n",
    "$$\n",
    "Gini(D) = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( D \\) is the dataset\n",
    "- \\( p_i \\) is the proportion of class \\( i \\) in the dataset\n",
    "- \\( n \\) is the number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e84290-3ee6-493f-9915-878730685244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62203d6-34ab-406b-8281-d4a683edc8f0",
   "metadata": {},
   "source": [
    "- It means that your lowest impure data will become decision node\n",
    "- We prefer less impure data for next splitting\n",
    "- We will make root node (for example company or Degree) in below example which will have:\n",
    "- **Low entropy**\n",
    "- **High information gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb102d-4817-4366-a22f-640a081bb690",
   "metadata": {},
   "source": [
    "<img src=\"Images/decision-tree-3.jpg\"  style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979aa8c-f4d9-4c8a-8a8c-7ed76567a8dc",
   "metadata": {},
   "source": [
    "- In the above example, the node that contains distinct number of either 1 or 0, it is **less impure**\n",
    "- So we will choose company as a root/parent node becuase it has high number of 1 and low number of 0\n",
    "- In other case i.e. Degree, number of 1 and 0 are equal, so we are not sure which value is true, so it is **more impure**\n",
    "- In above example we have low entropy in case of splitting through company, and\n",
    "- high entropy in case of splitting through Degree\n",
    "- So we will choose company as parent/root node for further splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eada7e9-93ff-4cc3-9c50-edf5d21a163d",
   "metadata": {},
   "source": [
    "So we will calculate entropies of:\n",
    "1) company\n",
    "2) Job\n",
    "3) Degree\n",
    "- And then decide to start splitting from node which should have **Lowest entropy** (impurity).\n",
    "- Low entropy means, **high information gain**.\n",
    "- And vice versa\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e58ea-22bf-4217-a858-4b1f11f6b795",
   "metadata": {},
   "source": [
    "Algo for doing above task:\n",
    "\n",
    "- **1st step:** We will calculate entropies of company, job, and degree. In this example, company has lowest entropy, so it will be first decision node, and we will split company into Amazon, Boat, Flipcard\n",
    "- **2nd step:** We will again calculate entropies of job and degree, and choose the node for furhter splitting which have lowest entropy, which is degree in this case\n",
    "- **3rd step:** Only one node left i.e., Job. This would be terminal/leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429e420-07a5-4c50-ab9a-c6557af9c246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
